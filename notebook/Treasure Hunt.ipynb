{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treasure Hunt\n",
    "\n",
    "\n",
    "Source: https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56\n",
    "\n",
    "## Introduction\n",
    "Reinforcement learning is a machine learning concept. Given an environment, the model attempts to understand the system with four basic principles. \n",
    "    1. States\n",
    "    2. Actions\n",
    "    3. Rewards\n",
    "    4. Policy\n",
    "\n",
    "An agent refers to who is learning the provided environment. This agent can encounter different types of states within this system which will help determine an action to select. Certain states provide various rewards which influence how our agent chooses some action. Lastly, the selection of actions is determined by some policy. \n",
    "\n",
    "In this notebook, we define a reinforcement learning model to select certain actions leveraging Q-learning as the policy.\n",
    "\n",
    "## Defining the System\n",
    "Treasure hunts consist of a brave hero, dangerous obstacles and glorious treasure respectively represented by our agent and various states. We also know our hero can move across the map to locate the treasure which represents our possible actions such as up, down, left and right. With our hero, we understand that dangerous obstacles represent a negative experience and treasures provide a positive experience. Thus the rewards are respectively negative and positive score values depending on what state our agent encounters. The following represents the list of defined system attributes.\n",
    "\n",
    "    1. Model/Environment\n",
    "       - A 2D matrix with indices representing locations with some state\n",
    "    2. States\n",
    "       - 0 represents a neutral block\n",
    "       - 1 represents an extremely dangerous forest\n",
    "       - 2 represents a monster\n",
    "       - 3 represents a small coin\n",
    "       - 4 represents the ultimate treasure room\n",
    "    3. Actions\n",
    "       - 0 is right\n",
    "       - 1 is left\n",
    "       - 2 is down\n",
    "       - 3 is up\n",
    "    4. Rewards\n",
    "       - Neutral blocks do not receive any rewards\n",
    "       - The forest blocks return -50 points\n",
    "       - Monsters damage points by 4\n",
    "       - Coins award 4 points\n",
    "       - Treasure room presents 50 points\n",
    "  \n",
    "## Policy\n",
    "For remembering and recording rewards given an action at some given state, Q-learning is leveraged. Essentially, the observed reward is saved based on the learning rate (e.g., how much the next step alters overall values) and gamma (e.g., opportunistic versus long-term reward) and the previously recorded reward. This is defined as follows:\n",
    "* Q[curState, action] += learnRate * (reward + gamma * np.max(Q[newState]) - Q[curState, action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the agent (Player) and system (Environment). These two classes work together.\n",
    "\n",
    "class Player():\n",
    "    def __init__(self, location=(0,0)):\n",
    "        # initialize player's location\n",
    "        self.Player=location\n",
    "        \n",
    "        # initialize score\n",
    "        self.Score=0\n",
    "    \n",
    "    def setScore(self, reward=0):\n",
    "        self.Score+=reward\n",
    "        \n",
    "    def setLocation(self, location):\n",
    "        self.Player=location\n",
    "        \n",
    "    def updateLocation(self, action):\n",
    "        # right=0, left=1, down=2, up=3\n",
    "        if action[0]==0:\n",
    "            self.Player=(self.Player[0],self.Player[1]+1)\n",
    "        elif action[0]==1:\n",
    "            self.Player=(self.Player[0],self.Player[1]-1)\n",
    "        elif action[0]==2:\n",
    "            self.Player=(self.Player[0]+1,self.Player[1])\n",
    "        elif action[0]==3:\n",
    "            self.Player=(self.Player[0]-1,self.Player[1])\n",
    "        \n",
    "    def getLocation(self):\n",
    "        return self.Player\n",
    "    \n",
    "    def getScore(self):\n",
    "        return self.Score\n",
    "        \n",
    "class Environment():    \n",
    "    def __init__(self,size=5):\n",
    "        # initialize matrix map\n",
    "        self.mapEnv = np.zeros(shape=(size,size))\n",
    "    \n",
    "    def getSize(self):\n",
    "        return self.mapEnv.shape[0]\n",
    "    \n",
    "    def setEnvironment(self):\n",
    "        # set states in map... for now this is static, but could alter\n",
    "        # [0 3 0 0 3]\n",
    "        # [1 0 1 1 1]\n",
    "        # [1 0 0 1 3]\n",
    "        # [1 0 0 0 2]\n",
    "        # [3 0 0 1 4]\n",
    "        \n",
    "        # 0 = blank space, 1 = wall/tree, 2 = monster, 3 = coins, 4 = treasure\n",
    "        self.mapEnv[0]=[0, 3, 0, 0, 3]\n",
    "        self.mapEnv[1]=[1, 0, 2, 1, 1]\n",
    "        self.mapEnv[2]=[1, 0, 0, 1, 3]\n",
    "        self.mapEnv[3]=[1, 0, 0, 0, 2]\n",
    "        self.mapEnv[4]=[3, 0, 0, 1, 4]\n",
    "        \n",
    "\n",
    "    def getState(self,location):\n",
    "        #remove tokens and monsters if encountered\n",
    "        state=int(self.mapEnv[location[0]][location[1]])\n",
    "        if state==2 or state==3: #monster or token\n",
    "            self.mapEnv[location[0]][location[1]]=0\n",
    "        return state\n",
    "\n",
    "    def getReward(self,state): # define what each state represents for scoring\n",
    "        if state == 0: #regular block\n",
    "            return 0\n",
    "        elif state == 1: #forest\n",
    "            return -50\n",
    "        elif state == 2: #monster\n",
    "            return -4\n",
    "        elif state == 3: #token\n",
    "            return 4\n",
    "        elif state == 4: #goal\n",
    "            return 50\n",
    "\n",
    "    def getPossibleActions(self,location):\n",
    "        # returns list of possible states/actions player can move\n",
    "    \n",
    "        # create list of up, down, left, right\n",
    "        # right=0, left=1, down=2, up=3\n",
    "        possibleMoveSet=[(0,location[1]+1), (1,location[1]-1), (2,location[0]+1), (3,location[0]-1)]\n",
    "\n",
    "        # filter list\n",
    "        #*must be equal or greater than 0 for both cases (e.g., can't go off the map in the bottom left)\n",
    "        #*must be equal or greater than the size for both cases (e.g., can't go off the map in the top right)\n",
    "        possibleMoveSet=[move for move in possibleMoveSet if move[1] > -1 if move[1] < self.mapEnv.shape[0]]\n",
    "        return possibleMoveSet\n",
    "    \n",
    "    def printOut(self,location, action=None):\n",
    "        #now print out action taken\n",
    "        # right=0, left=1, down=2, up=3\n",
    "        if action==None:\n",
    "            print(\"Start\")\n",
    "        elif action[0]==0:\n",
    "            print(\"Action taken: Right\")\n",
    "        elif action[0]==1:\n",
    "            print(\"Action taken: Left\")\n",
    "        elif action[0]==2:\n",
    "            print(\"Action taken: Down\")\n",
    "        elif action[0]==3:\n",
    "            print(\"Action taken: Up\")\n",
    "        \n",
    "        # Map representation\n",
    "        # [0 3 0 0 3]\n",
    "        # [1 0 1 1 1]\n",
    "        # [1 0 0 1 3]\n",
    "        # [1 0 0 0 2]\n",
    "        # [3 0 0 1 4]\n",
    "        \n",
    "        newMap=self.mapEnv.copy()\n",
    "        newMap[location[0],location[1]] = 9\n",
    "        print(newMap)\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "The end goal is the treasure room labelled as state 4. Thus, we must loop until the agent locates the reward. For each episode, an action is chosen and the map is printed with the new location of the agent (labelled 9). \n",
    "\n",
    "An important aspect to point out is exploration versus exploitation. To better understand certain actions at some reward, we must explore, but in order to get the best rewards, we typically want to exploit the actions that provide the highest possible reward. This is defined based on what we want and can be altered; however, for the intents and purposes of this course, I selected 0.6 such that upon random selection of a variable from range 0 to 1, will determine if a random action is chosen or based on our Q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(gameMap, player, Q):\n",
    "    # Establish parameters for updating Q table and exploration/exploitation\n",
    "    explorExploit=.6 #explore and exploit values\n",
    "    learnRate=0.4 #based on https://en.wikipedia.org/wiki/Q-learning\n",
    "    gamma=0.2 #determines focus on short term versus long term scores\n",
    "\n",
    "    print(\"----- Iteration 0 -----\")\n",
    "    print(\"Player Score: \"+str(player.getScore()))\n",
    "    gameMap.printOut(player.getLocation()) #print out beginning\n",
    "\n",
    "    curLoc= player.getLocation() # Get current location\n",
    "    curState= gameMap.getState(curLoc) # Get current state\n",
    "    iteration=0\n",
    "    while curState!=4 and iteration!=-1: #loop until goal is reached or break if not located\n",
    "        curActions= gameMap.getPossibleActions(curLoc) # Get possible actions\n",
    "        eps=np.random.uniform(0, 1) # generate random value from 0 to 1\n",
    "        if eps>=explorExploit: # grab random possible action for exploration\n",
    "            index = np.random.randint(low=0,high=len(curActions), size=1)[0] #if returned list of multiple values, select the first\n",
    "            action=curActions[index] # randomly grab an action\n",
    "        else: # exploit highest value!\n",
    "            #out of all possible actions, grab the one with the highest Q value\n",
    "            chosen=[(-1,-1),float(\"-inf\")] #format: ((action),Q-value)\n",
    "            for act in curActions:\n",
    "                if Q[curState,act[0]]>chosen[1]:\n",
    "                    chosen=[act,Q[curState,act[0]]]\n",
    "            action=chosen[0] #grab highest value\n",
    "\n",
    "        # with the selected action, update the player's location\n",
    "        player.updateLocation(action) # set the location to selected action\n",
    "        newState=gameMap.getState(player.getLocation()) # grab the new state\n",
    "        reward=gameMap.getReward(newState)\n",
    "        player.setScore(reward) # set the score\n",
    "\n",
    "        # Updating Q values\n",
    "        Q[curState, action[0]] = Q[curState, action[0]] + learnRate * (reward + gamma * np.max(Q[newState]) - Q[curState, action[0]])\n",
    "\n",
    "        #Increase iteration to prevent infinite loop\n",
    "        iteration+=1\n",
    "\n",
    "        #print out each change\n",
    "        print(\"----- Iteration \"+ str(iteration) + \" -----\")\n",
    "        print(\"Player Score: \"+str(player.getScore()))\n",
    "        gameMap.printOut(player.getLocation(),action)\n",
    "\n",
    "        #get current location and state\n",
    "        curLoc= player.getLocation() # Get current location\n",
    "        curState= gameMap.getState(curLoc) # Get current state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting\n",
    "The next few cells showcase how to setup the environment and player as well as implement the simulation with a few function calls. Q is established and passed prior as we want to check how the agent acts after learning the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Iteration 0 -----\n",
      "Player Score: 0\n",
      "Start\n",
      "[[9. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 1 -----\n",
      "Player Score: 4\n",
      "Action taken: Right\n",
      "[[0. 9. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 2 -----\n",
      "Player Score: 4\n",
      "Action taken: Right\n",
      "[[0. 0. 9. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 3 -----\n",
      "Player Score: 4\n",
      "Action taken: Left\n",
      "[[0. 9. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 4 -----\n",
      "Player Score: 4\n",
      "Action taken: Right\n",
      "[[0. 0. 9. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 5 -----\n",
      "Player Score: 4\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 9. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 6 -----\n",
      "Player Score: 8\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 0. 9.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 7 -----\n",
      "Player Score: -42\n",
      "Action taken: Down\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 9.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 8 -----\n",
      "Player Score: -92\n",
      "Action taken: Left\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 9. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 9 -----\n",
      "Player Score: -142\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 9.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 10 -----\n",
      "Player Score: -138\n",
      "Action taken: Down\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 9.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 11 -----\n",
      "Player Score: -188\n",
      "Action taken: Left\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 12 -----\n",
      "Player Score: -188\n",
      "Action taken: Down\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 9. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 13 -----\n",
      "Player Score: -192\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 9.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 14 -----\n",
      "Player Score: -192\n",
      "Action taken: Up\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 9.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 15 -----\n",
      "Player Score: -242\n",
      "Action taken: Up\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 9.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 16 -----\n",
      "Player Score: -242\n",
      "Action taken: Down\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 9.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 17 -----\n",
      "Player Score: -292\n",
      "Action taken: Left\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 18 -----\n",
      "Player Score: -292\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 9.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 19 -----\n",
      "Player Score: -342\n",
      "Action taken: Up\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 9.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 20 -----\n",
      "Player Score: -342\n",
      "Action taken: Down\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 9.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 21 -----\n",
      "Player Score: -392\n",
      "Action taken: Left\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 22 -----\n",
      "Player Score: -392\n",
      "Action taken: Down\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 23 -----\n",
      "Player Score: -392\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 9.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 24 -----\n",
      "Player Score: -392\n",
      "Action taken: Left\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 25 -----\n",
      "Player Score: -392\n",
      "Action taken: Left\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 9. 0. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 26 -----\n",
      "Player Score: -392\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 27 -----\n",
      "Player Score: -392\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 9.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 28 -----\n",
      "Player Score: -392\n",
      "Action taken: Left\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 29 -----\n",
      "Player Score: -392\n",
      "Action taken: Right\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 9.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 30 -----\n",
      "Player Score: -342\n",
      "Action taken: Down\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 9.]]\n"
     ]
    }
   ],
   "source": [
    "# Setup environment and place player\n",
    "gameMap = Environment()\n",
    "gameMap.setEnvironment()\n",
    "player = Player()\n",
    "Q = np.zeros((5, 4)) # Initialize q-table values to 0. First four represent the four states and the second four represents the directions to move\n",
    "\n",
    "simulate(gameMap, player, Q)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -0.09811639  -8.47625251   8.         -31.89617063]\n",
      " [-12.03671097 -20.           0.16827054   0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [  0.           0.           0.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Iteration 0 -----\n",
      "Player Score: 0\n",
      "Start\n",
      "[[9. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 1 -----\n",
      "Player Score: -50\n",
      "Action taken: Down\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [9. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 2 -----\n",
      "Player Score: -100\n",
      "Action taken: Down\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [9. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 3 -----\n",
      "Player Score: -100\n",
      "Action taken: Right\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 9. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 4 -----\n",
      "Player Score: -100\n",
      "Action taken: Right\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 9. 1. 3.]\n",
      " [1. 0. 0. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 5 -----\n",
      "Player Score: -100\n",
      "Action taken: Down\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 9. 0. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 6 -----\n",
      "Player Score: -100\n",
      "Action taken: Right\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 9. 2.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 7 -----\n",
      "Player Score: -104\n",
      "Action taken: Right\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 9.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 8 -----\n",
      "Player Score: -104\n",
      "Action taken: Left\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 9 -----\n",
      "Player Score: -104\n",
      "Action taken: Right\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 9.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 10 -----\n",
      "Player Score: -104\n",
      "Action taken: Left\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 11 -----\n",
      "Player Score: -104\n",
      "Action taken: Right\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 9.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 12 -----\n",
      "Player Score: -104\n",
      "Action taken: Left\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 9. 0.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 13 -----\n",
      "Player Score: -104\n",
      "Action taken: Right\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 9.]\n",
      " [3. 0. 0. 1. 4.]]\n",
      "----- Iteration 14 -----\n",
      "Player Score: -54\n",
      "Action taken: Down\n",
      "[[0. 3. 0. 0. 3.]\n",
      " [1. 0. 2. 1. 1.]\n",
      " [1. 0. 0. 1. 3.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [3. 0. 0. 1. 9.]]\n"
     ]
    }
   ],
   "source": [
    "#Reset player and use the new calculated Q values\n",
    "gameMap2 = Environment()\n",
    "gameMap2.setEnvironment()\n",
    "player2 = Player()\n",
    "\n",
    "simulate(gameMap2, player2, Q)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
